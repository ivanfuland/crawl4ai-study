import asyncio
import json
import os
import re
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from playwright.async_api import Page, BrowserContext, async_playwright
from crawl4ai.content_filter_strategy import PruningContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from dotenv import load_dotenv
import datetime

# åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# å¼ºåˆ¶è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["PLAYWRIGHT_BROWSER_VISIBLE"] = "1"
os.environ["PLAYWRIGHT_FORCE_VISIBLE"] = "1"  # é¢å¤–å¼ºåˆ¶å¯è§


# å…¨å±€é…ç½® - ä»ç¯å¢ƒå˜é‡ä¸­è¯»å–
TARGET_URL = os.getenv("TARGET_URL")
COOKIE_PATH = os.getenv("COOKIE_PATH")
LOGIN_URL = os.getenv("LOGIN_URL")
LOGIN_TIMEOUT = int(os.getenv("LOGIN_TIMEOUT"))  # ç™»å½•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
DEBUG = False

# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_FILE_PATH = "output/site_configs.json"

# åŠ è½½æˆ–åˆ›å»ºç½‘ç«™é…ç½®
def load_or_create_site_configs():
    try:
        # ç¡®ä¿outputç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(CONFIG_FILE_PATH), exist_ok=True)
        
        # å°è¯•è¯»å–é…ç½®æ–‡ä»¶
        if os.path.exists(CONFIG_FILE_PATH):
            print(f"[CONFIG] æ­£åœ¨ä» {CONFIG_FILE_PATH} è¯»å–ç½‘ç«™é…ç½®")
            with open(CONFIG_FILE_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
        
        # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥æŠ¥é”™
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ {CONFIG_FILE_PATH} ä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»ºé…ç½®æ–‡ä»¶")
        
    except Exception as e:
        # ä¸å†è¿”å›é»˜è®¤é…ç½®ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
        raise Exception(f"åŠ è½½é…ç½®æ–‡ä»¶å‡ºé”™: {e}")

# åŠ è½½ç½‘ç«™é…ç½®
SITE_CONFIGS = load_or_create_site_configs()

# æ‰“å°å½“å‰é…ç½®
print("[CONFIG] å½“å‰é…ç½®:")
print(f"[CONFIG] TARGET_URL: {TARGET_URL}")
print(f"[CONFIG] COOKIE_PATH: {COOKIE_PATH}")
print(f"[CONFIG] LOGIN_URL: {len(LOGIN_URL) > 20 and LOGIN_URL[:20]+'...' or LOGIN_URL}")
print(f"[CONFIG] DEBUG: {DEBUG}")
print(f"[CONFIG] é…ç½®æ–‡ä»¶: {CONFIG_FILE_PATH}")
print(f"[CONFIG] å·²åŠ è½½ç½‘ç«™é…ç½®: {', '.join([domain for domain in SITE_CONFIGS.keys() if domain != 'default'])}")
if DEBUG:
    print("[CONFIG] ç½‘ç«™é…ç½®è¯¦æƒ…:")
    for domain, config in SITE_CONFIGS.items():
        print(f"  - {domain}:")
        for key, value in config.items():
            if isinstance(value, list) and len(value) > 3:
                print(f"    {key}: [{value[0]}, {value[1]}, ... +{len(value)-2}é¡¹]")
            else:
                print(f"    {key}: {value}")

# ä¿å­˜ç½‘ç«™é…ç½®
def save_site_configs(configs):
    try:
        with open(CONFIG_FILE_PATH, "w", encoding="utf-8") as f:
            json.dump(configs, f, ensure_ascii=False, indent=2)
        print(f"[CONFIG] å·²ä¿å­˜ç½‘ç«™é…ç½®åˆ° {CONFIG_FILE_PATH}")
        return True
    except Exception as e:
        print(f"[CONFIG] ä¿å­˜é…ç½®æ–‡ä»¶å‡ºé”™: {e}")
        return False

async def manual_login():
    """ä½¿ç”¨ç›´æ¥çš„playwrightæ–¹å¼æ‰“å¼€æµè§ˆå™¨ç™»å½•ï¼Œå¹¶æš‚åœç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨æ“ä½œã€‚"""
    print("[LOGIN] æ­£åœ¨å¯åŠ¨ç›´æ¥æµè§ˆå™¨ç™»å½•...")
    
    # ç¡®ä¿outputç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(COOKIE_PATH), exist_ok=True)
    
    async with async_playwright() as playwright:
        # å¯åŠ¨æµè§ˆå™¨
        browser = await playwright.chromium.launch(
            headless=False,
            args=[
                "--disable-gpu",
                "--no-sandbox",
                "--disable-web-security",
                "--window-size=1280,800"
            ]
        )
        
        # åˆ›å»ºä¸Šä¸‹æ–‡
        context = await browser.new_context(
            viewport={"width": 1280, "height": 800},
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
        )
        
        # æ‰“å¼€é¡µé¢
        page = await context.new_page()
        print("[LOGIN] æˆåŠŸåˆ›å»ºæµè§ˆå™¨çª—å£")
        
        # è®¿é—®ç™»å½•é¡µé¢
        print(f"[LOGIN] æ­£åœ¨å¯¼èˆªåˆ°ç™»å½•é¡µé¢: {LOGIN_URL}")
        await page.goto(LOGIN_URL)
        await page.wait_for_load_state("networkidle")
        print("[LOGIN] å·²åŠ è½½ç™»å½•é¡µé¢")
        
        # >>> æš‚åœç­‰å¾…æ‰‹åŠ¨ç™»å½• <<<
        print("\n>>> [LOGIN] è„šæœ¬å·²æš‚åœï¼Œè¯·åœ¨å¼¹å‡ºçš„æµè§ˆå™¨çª—å£ä¸­å®Œæˆç™»å½•æ“ä½œ.<<<")
        print(">>> å®Œæˆç™»å½•åï¼Œè¯·å›åˆ°è¿™é‡Œï¼ŒæŒ‰ 'Resume' (æˆ–ç±»ä¼¼æŒ‰é’®) æ¢å¤è„šæœ¬æ‰§è¡Œ.<<<\n")
        await page.pause() 
        # ç”¨æˆ·åœ¨æ­¤å¤„æ‰‹åŠ¨ç™»å½•ï¼Œç„¶åæ‰‹åŠ¨æ¢å¤è„šæœ¬

        print("[LOGIN] è„šæœ¬å·²æ¢å¤æ‰§è¡Œï¼Œæ­£åœ¨æ£€æŸ¥ç™»å½•çŠ¶æ€...")
        login_success = False
        try:
            # --- ä»é…ç½®ä¸­è·å–ç™»å½•æˆåŠŸæŒ‡æ ‡ ---
            # é¦–å…ˆå°è¯•ç¡®å®šå½“å‰é¡µé¢å¯¹åº”å“ªä¸ªåŸŸåé…ç½®
            current_domain = None
            for domain in SITE_CONFIGS.keys():
                if domain != "default" and domain in page.url:
                    current_domain = domain
                    break
            
            # è·å–å¯¹åº”çš„ç™»å½•æˆåŠŸæŒ‡æ ‡
            login_indicators = None
            if current_domain:
                login_indicators = SITE_CONFIGS[current_domain].get("login_success_indicators", {})
                print(f"[LOGIN CHECK] ä½¿ç”¨ {current_domain} çš„ç™»å½•æ£€æµ‹è§„åˆ™")
            else:
                # å¦‚æœæ‰¾ä¸åˆ°åŒ¹é…çš„åŸŸåé…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
                login_indicators = SITE_CONFIGS["default"].get("login_success_indicators", {})
                print("[LOGIN CHECK] æœªæ‰¾åˆ°åŒ¹é…çš„åŸŸåé…ç½®ï¼Œä½¿ç”¨é»˜è®¤ç™»å½•æ£€æµ‹è§„åˆ™")
            
            # å¯¹URLè¿›è¡Œæ£€æŸ¥
            url_check_passed = True
            
            # æ£€æŸ¥URLåŒ…å«é¡¹
            for url_fragment in login_indicators.get("url_contains", []):
                if url_fragment not in page.url:
                    print(f"[LOGIN CHECK] URLä¸åŒ…å« '{url_fragment}'")
                    url_check_passed = False
                    break
                else:
                    print(f"[LOGIN CHECK] URLåŒ…å« '{url_fragment}' âœ“")
            
            # æ£€æŸ¥URLä¸åŒ…å«é¡¹
            for url_fragment in login_indicators.get("url_not_contains", []):
                if url_fragment in page.url:
                    print(f"[LOGIN CHECK] URLåŒ…å«è¢«æ’é™¤çš„å­—ç¬¦ä¸² '{url_fragment}'")
                    url_check_passed = False
                    break
                else:
                    print(f"[LOGIN CHECK] URLä¸åŒ…å«æ’é™¤é¡¹ '{url_fragment}' âœ“")
            
            # å¦‚æœURLæ£€æŸ¥é€šè¿‡ï¼Œæ£€æŸ¥DOMå…ƒç´ 
            if url_check_passed:
                elements_check_passed = True
                for selector in login_indicators.get("elements_exist", []):
                    element = await page.query_selector(selector)
                    if not element:
                        print(f"[LOGIN CHECK] æœªæ‰¾åˆ°å…ƒç´  '{selector}'")
                        elements_check_passed = False
                        break
                    else:
                        print(f"[LOGIN CHECK] æ‰¾åˆ°å…ƒç´  '{selector}' âœ“")
                
                # ç»¼åˆåˆ¤æ–­
                login_success = elements_check_passed

        except Exception as e:
            print(f"[LOGIN CHECK] æ£€æŸ¥ç™»å½•çŠ¶æ€æ—¶å‡ºé”™: {e}")
            login_success = False # å‡ºé”™åˆ™è®¤ä¸ºæœªç™»å½•
        
        if login_success:
            print("[LOGIN] æ£€æµ‹åˆ°ç™»å½•æˆåŠŸï¼ä¿å­˜cookie...")
            cookies = await context.cookies()
            # ç¡®ä¿outputç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(COOKIE_PATH), exist_ok=True)
            with open(COOKIE_PATH, "w", encoding="utf-8") as f:
                json.dump(cookies, f, ensure_ascii=False, indent=2)
            print(f"[LOGIN] å·²ä¿å­˜cookiesåˆ° {COOKIE_PATH}")
            
            await browser.close()
            print("[LOGIN] æµè§ˆå™¨å·²å…³é—­")
            return True
        else:
            print("[LOGIN] æœªèƒ½ç¡®è®¤ç™»å½•æˆåŠŸï¼Œè¯·æ£€æŸ¥ç™»å½•æ“ä½œæˆ–ç™»å½•æˆåŠŸåˆ¤æ–­é€»è¾‘")
            await browser.close()
            print("[LOGIN] æµè§ˆå™¨å·²å…³é—­")
            return False

# é€šç”¨çš„ç™»å½•çŠ¶æ€æ£€æŸ¥å‡½æ•°
async def check_login_status(page, domain=None):
    """
    æ ¹æ®é…ç½®æ£€æŸ¥é¡µé¢æ˜¯å¦å¤„äºç™»å½•çŠ¶æ€
    Args:
        page: playwrighté¡µé¢å¯¹è±¡
        domain: å¯é€‰ï¼ŒæŒ‡å®šåŸŸåï¼Œå¦‚ä¸æŒ‡å®šåˆ™ä»URLè‡ªåŠ¨åˆ¤æ–­
    Returns:
        bool: æ˜¯å¦å·²ç™»å½•
    """
    try:
        # å¦‚æœæœªæŒ‡å®šåŸŸåï¼Œå°è¯•ä»URLä¸­æå–
        if not domain:
            for config_domain in SITE_CONFIGS.keys():
                if config_domain != "default" and config_domain in page.url:
                    domain = config_domain
                    break
        
        # è·å–ç™»å½•æŒ‡æ ‡
        login_indicators = None
        if domain and domain in SITE_CONFIGS:
            login_indicators = SITE_CONFIGS[domain].get("login_success_indicators", {})
        else:
            login_indicators = SITE_CONFIGS["default"].get("login_success_indicators", {})
        
        # éªŒè¯URLåŒ…å«é¡¹
        for url_fragment in login_indicators.get("url_contains", []):
            if url_fragment not in page.url:
                return False
        
        # éªŒè¯URLä¸åŒ…å«é¡¹
        for url_fragment in login_indicators.get("url_not_contains", []):
            if url_fragment in page.url:
                return False
        
        # éªŒè¯DOMå…ƒç´ å­˜åœ¨
        for selector in login_indicators.get("elements_exist", []):
            element = await page.query_selector(selector)
            if not element:
                return False
                
        # æ‰€æœ‰æ£€æŸ¥é€šè¿‡
        return True
    except Exception as e:
        print(f"[LOGIN CHECK] æ£€æŸ¥ç™»å½•çŠ¶æ€å‡ºé”™: {e}")
        return False  # å‡ºé”™æ—¶é»˜è®¤ä¸ºæœªç™»å½•

async def main():
    """åŸçˆ¬è™«ä¸»å‡½æ•°"""
    print("ğŸ”— ç®€åŒ–ç‰ˆçˆ¬è™«ï¼šåˆ©ç”¨crawl4aiåŸç”ŸåŠŸèƒ½ï¼Œä¿ç•™cookieéªŒè¯")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰‹åŠ¨ç™»å½•
    if not os.path.exists(COOKIE_PATH):
        print(f"[MAIN] Cookieæ–‡ä»¶ä¸å­˜åœ¨äº {COOKIE_PATH}ï¼Œéœ€è¦å…ˆè¿›è¡Œæ‰‹åŠ¨ç™»å½•")
        login_success = await manual_login()
        if not login_success:
            print("[MAIN] ç™»å½•å¤±è´¥ï¼Œè¯·é‡è¯•")
            return
    
    # 1) é…ç½®æµè§ˆå™¨
    browser_config = BrowserConfig(
        headless=False,  # å¯è§†æ¨¡å¼ï¼Œä¾¿äºè°ƒè¯•
        viewport={"width": 1280, "height": 800},
        verbose=True
    )

    # 2) é…ç½®çˆ¬è™«è¿è¡Œå‚æ•°
    crawler_run_config = CrawlerRunConfig(
        js_code="""
            // å°è¯•å±•å¼€å†…å®¹å’Œç§»é™¤é®æŒ¡
            function enhancePage() {
                // ç‚¹å‡»å±•å¼€æŒ‰é’®
                const expandButtons = document.querySelectorAll('.read-more, .expand, .show-more, .view-all, button:contains("æŸ¥çœ‹æ›´å¤š")');
                for (const button of expandButtons) {
                    button.click();
                }
                
                // æ»šåŠ¨é¡µé¢è§¦å‘åŠ è½½
                window.scrollTo(0, document.body.scrollHeight);
                
                // ç§»é™¤é®æŒ¡å±‚
                const overlays = document.querySelectorAll('.overlay, .mask, .modal, .login-modal');
                for (const overlay of overlays) {
                    overlay.remove();
                }
                
                // ç¡®ä¿å¯ä»¥æ»šåŠ¨
                document.body.style.overflow = 'auto';               
            }
            
            // æ‰§è¡Œé¡µé¢å¢å¼º
            enhancePage();
            setTimeout(enhancePage, 3000); // å¢åŠ å»¶æ—¶è‡³3ç§’ï¼Œç»™å†…å®¹æ›´å¤šåŠ è½½æ—¶é—´
        """,
        wait_for="body", 
        cache_mode=CacheMode.BYPASS,
        markdown_generator=DefaultMarkdownGenerator(
            # ä½¿ç”¨åŸºæœ¬é…ç½®
            content_filter=PruningContentFilter(),
            options={
                "ignore_links": False,
                "escape_html": True
            }
        )
    )

    # 3) åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = AsyncWebCrawler(config=browser_config)

    # ç™»å½•çŠ¶æ€ç®¡ç†
    login_status = {
        "is_logged_in": False,
        "cookies_loaded": False
    }

    # Hook: é¡µé¢å’Œä¸Šä¸‹æ–‡åˆ›å»ºå
    async def on_page_context_created(page: Page, context: BrowserContext, **kwargs):
        print("[HOOK] è®¾ç½®é¡µé¢å’Œä¸Šä¸‹æ–‡...")
        
        # è®¾ç½®è¶…æ—¶
        page.set_default_timeout(60000)
        
        # å°è¯•åŠ è½½å·²ä¿å­˜çš„cookie
        if os.path.exists(COOKIE_PATH):
            try:
                with open(COOKIE_PATH, "r", encoding="utf-8") as f:
                    cookies = json.load(f)
                    if cookies and len(cookies) > 0:
                        await context.add_cookies(cookies)
                        print(f"[HOOK] æˆåŠŸåŠ è½½ {len(cookies)} ä¸ªcookie")
                        login_status["cookies_loaded"] = True
                        # ä¸å†å‡è®¾cookieä¸€å®šæœ‰æ•ˆï¼Œå°†åœ¨é¡µé¢åŠ è½½åæ£€æŸ¥
                        login_status["is_logged_in"] = False  # åˆå§‹è®¾ä¸ºFalseï¼Œç­‰é¡µé¢åŠ è½½åéªŒè¯
                        print("[HOOK] Cookieå·²åŠ è½½ï¼Œç™»å½•çŠ¶æ€å°†åœ¨é¡µé¢åŠ è½½åéªŒè¯")
                    else:
                        print("[HOOK] Cookieæ–‡ä»¶å­˜åœ¨ä½†ä¸ºç©ºæˆ–æ— æ•ˆ")
            except Exception as e:
                print(f"[HOOK] åŠ è½½Cookieå¤±è´¥: {str(e)}")
        
        return page

    # Hook: é¡µé¢å¯¼èˆªå‰
    async def before_goto(page: Page, context: BrowserContext, url: str, **kwargs):
        print(f"[HOOK] å‡†å¤‡è®¿é—®: {url}")
        
        # åŸºæœ¬HTTPå¤´
        headers = {
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
        }
        
        # è®¾ç½®HTTPå¤´
        await page.set_extra_http_headers(headers)
        
        return page

    # Hook: é¡µé¢å¯¼èˆªå
    async def after_goto(page: Page, context: BrowserContext, url: str, response, **kwargs):
        print(f"[HOOK] å·²åŠ è½½é¡µé¢: {url}")
        
        # æå–åŸŸå
        domain = None
        match = re.search(r'https?://(?:www\.)?([^/]+)', url)
        if match:
            domain = match.group(1)
        
        # æ ¹æ®URLç¡®å®šä½¿ç”¨å“ªä¸ªç½‘ç«™é…ç½®
        site_config = None
        matched_domain = None
        for config_domain, config in SITE_CONFIGS.items():
            if config_domain != "default" and config_domain in url:
                site_config = config
                matched_domain = config_domain
                print(f"[HOOK] ä½¿ç”¨ {config_domain} çš„é…ç½®")
                break
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…çš„é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        if site_config is None:
            site_config = SITE_CONFIGS["default"]
            print(f"[HOOK] æœªæ‰¾åˆ°åŒ¹é…çš„åŸŸåé…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            # æå–å½“å‰åŸŸåä»…ç”¨äºæ—¥å¿—è®°å½•
            if domain:
                print(f"[HOOK] å½“å‰åŸŸå: {domain} (æœªé…ç½®)")
        
        try:
            # ç­‰å¾…é¡µé¢åŠ è½½
            wait_for_load = site_config.get("wait_for_load", "domcontentloaded")
            timeout = site_config.get("timeout", 10000)
            print(f"[HOOK] ç­‰å¾…é¡µé¢åŠ è½½ ({wait_for_load})ï¼Œè¶…æ—¶: {timeout}ms")
            await page.wait_for_load_state(wait_for_load, timeout=timeout)
            
            # æ£€æŸ¥å†…å®¹æ˜¯å¦å¯è§
            content_selectors = site_config.get("content_selectors", [])
            if content_selectors:
                selector_str = ", ".join(content_selectors)
                content_visible = await page.is_visible(selector_str, timeout=3000)
                print(f"[HOOK] å†…å®¹å¯è§æ€§æ£€æŸ¥: {content_visible}")
                
                # å¦‚æœé…ç½®äº†éœ€è¦åˆ·æ–°æ£€æŸ¥ä¸”å†…å®¹ä¸å¯è§ï¼Œå°è¯•åˆ·æ–°
                if not content_visible and site_config.get("needs_refresh_check", False) and login_status["cookies_loaded"]:
                    print("[HOOK] å†…å®¹ä¸å¯è§ï¼Œå°è¯•åˆ·æ–°é¡µé¢...")
                    await page.reload()
                    await page.wait_for_load_state(wait_for_load, timeout=timeout)
                    
                    # å†æ¬¡æ£€æŸ¥å†…å®¹å¯è§æ€§
                    content_visible = await page.is_visible(selector_str, timeout=3000)
                    if not content_visible:
                        print("[HOOK] åˆ·æ–°åä»æ— æ³•çœ‹åˆ°å†…å®¹ï¼Œæ£€æŸ¥ç™»å½•çŠ¶æ€...")
                        # ä½¿ç”¨é€šç”¨ç™»å½•æ£€æŸ¥å‡½æ•°
                        login_status["is_logged_in"] = await check_login_status(page, matched_domain)
                        if not login_status["is_logged_in"]:
                            print("[HOOK] ç™»å½•æ£€æŸ¥å¤±è´¥ï¼ŒCookieå¯èƒ½å·²å¤±æ•ˆ")
                        else:
                            print("[HOOK] ç™»å½•çŠ¶æ€æ­£å¸¸ï¼Œä½†å†…å®¹ä»ç„¶ä¸å¯è§")
            
            # é¢å¤–ç­‰å¾…æ—¶é—´
            extra_wait = site_config.get("extra_wait", 0)
            if extra_wait > 0:
                print(f"[HOOK] é¢å¤–ç­‰å¾… {extra_wait} ç§’...")
                await asyncio.sleep(extra_wait)
            
            # æ‰§è¡Œæ»šåŠ¨è¡Œä¸º
            scroll_behavior = site_config.get("scroll_behavior", None)
            if scroll_behavior:
                print(f"[HOOK] æ‰§è¡Œæ»šåŠ¨è¡Œä¸º: {scroll_behavior}")
                if scroll_behavior == "full":
                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
                elif scroll_behavior == "half_then_full":
                    await page.evaluate("""
                        window.scrollTo(0, document.body.scrollHeight / 2);
                        setTimeout(() => {
                            window.scrollTo(0, document.body.scrollHeight);
                        }, 1000);
                    """)
                await asyncio.sleep(2)  # æ»šåŠ¨åç­‰å¾…
            
            # è°ƒè¯•æ¨¡å¼ä¸‹æˆªå›¾
            if DEBUG and site_config.get("screenshot_debug", False):
                filename = url.split("//")[-1].replace("/", "_")[:30]
                screenshot_path = f"debug_{filename}.png"
                print(f"[HOOK] ä¿å­˜é¡µé¢æˆªå›¾åˆ° {screenshot_path}")
                await page.screenshot(path=screenshot_path, full_page=True)
                
        except Exception as e:
            print(f"[HOOK] å¤„ç†é¡µé¢æ—¶å‡ºé”™: {e}")
            # ç»§ç»­æ‰§è¡Œï¼Œä¸ä¸­æ–­æµç¨‹
        
        return page

    # æ³¨å†Œé’©å­å‡½æ•°
    crawler.crawler_strategy.set_hook("on_page_context_created", on_page_context_created)
    crawler.crawler_strategy.set_hook("before_goto", before_goto)
    crawler.crawler_strategy.set_hook("after_goto", after_goto)

    # å¯åŠ¨çˆ¬è™«
    await crawler.start()

    # è¿è¡Œçˆ¬è™«è®¿é—®ç›®æ ‡URL
    result = await crawler.arun(TARGET_URL, config=crawler_run_config)

    if result.success:
        print("\nçˆ¬å–æˆåŠŸï¼")
        print("URL:", result.url)
        print("HTMLé•¿åº¦:", len(result.html))
        print(f"ç™»å½•çŠ¶æ€: {'å·²ç™»å½•' if login_status['is_logged_in'] else 'æœªç™»å½•'}")
        
        # ä»metadataè·å–æ ‡é¢˜
        page_title = "æœªçŸ¥æ ‡é¢˜"
        if hasattr(result, 'metadata') and result.metadata and result.metadata.get("title"):
            page_title = result.metadata.get("title")
            print(f"é¡µé¢æ ‡é¢˜: {page_title}")
        
        # å¤„ç†é¡µé¢æ ‡é¢˜ï¼Œæ›¿æ¢éæ³•å­—ç¬¦ï¼Œç”¨äºåˆ›å»ºæ–‡ä»¶å¤¹
        safe_title = "".join([c if c.isalnum() or c in [' ', '_', '-'] else '_' for c in page_title])
        safe_title = safe_title.strip()
        if len(safe_title) > 50:
            safe_title = safe_title[:50]
        
        # æ·»åŠ æ—¶é—´æˆ³ç¡®ä¿å”¯ä¸€æ€§
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_title = f"{safe_title}_{timestamp}" if safe_title else f"unnamed_page_{timestamp}"
        
        # æ ¹æ®é¡µé¢æ ‡é¢˜åˆ›å»ºç›®å½•
        title_dir = os.path.join("output", safe_title)
        os.makedirs(title_dir, exist_ok=True)
        print(f"[OUTPUT] åˆ›å»ºç›®å½•: {title_dir}")
        
        # ä¿å­˜HTMLç»“æœ
        html_path = os.path.join(title_dir, "crawled_content.html")
        with open(html_path, "w", encoding="utf-8") as f:
            f.write(result.html)
            print(f"å·²ä¿å­˜HTMLå†…å®¹åˆ° {html_path}")        
        
        # ä¿å­˜Markdownç»“æœ
        md_path = os.path.join(title_dir, "crawled_content.md")
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(result.markdown.fit_markdown)
            print(f"å·²ä¿å­˜Markdownå†…å®¹åˆ° {md_path}")
    else:
        print("çˆ¬å–å¤±è´¥:", result.error_message)

    await crawler.close()

if __name__ == "__main__":
    asyncio.run(main()) 