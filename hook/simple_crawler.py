import asyncio
import json
import os
import re
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from playwright.async_api import Page, BrowserContext
from crawl4ai.content_filter_strategy import PruningContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

# å…¨å±€é…ç½®
TARGET_URL = "https://articles.zsxq.com/id_m5c8015ehlem.html"
COOKIE_PATH = "zsxq_cookies.json"
LOGIN_URL = "https://wx.zsxq.com/dweb2/login"
LOGIN_TIMEOUT = 60  # ç™»å½•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
DEBUG = True

async def main():
    print("ğŸ”— ç®€åŒ–ç‰ˆçˆ¬è™«ï¼šåˆ©ç”¨crawl4aiåŸç”ŸåŠŸèƒ½ï¼Œä¿ç•™cookieéªŒè¯")

    # 1) é…ç½®æµè§ˆå™¨
    browser_config = BrowserConfig(
        headless=False,  # å¯è§†æ¨¡å¼ï¼Œä¾¿äºè°ƒè¯•
        viewport={"width": 1280, "height": 800},
        verbose=True
    )

    # 2) é…ç½®çˆ¬è™«è¿è¡Œå‚æ•°
    crawler_run_config = CrawlerRunConfig(
        js_code="""
            // å°è¯•å±•å¼€å†…å®¹å’Œç§»é™¤é®æŒ¡
            function enhancePage() {
                // ç‚¹å‡»å±•å¼€æŒ‰é’®
                const expandButtons = document.querySelectorAll('.read-more, .expand, .show-more, .view-all, button:contains("æŸ¥çœ‹æ›´å¤š")');
                for (const button of expandButtons) {
                    button.click();
                }
                
                // æ»šåŠ¨é¡µé¢è§¦å‘åŠ è½½
                window.scrollTo(0, document.body.scrollHeight);
                
                // ç§»é™¤é®æŒ¡å±‚
                const overlays = document.querySelectorAll('.overlay, .mask, .modal, .login-modal');
                for (const overlay of overlays) {
                    overlay.remove();
                }
                
                // ç¡®ä¿å¯ä»¥æ»šåŠ¨
                document.body.style.overflow = 'auto';
            }
            
            // æ‰§è¡Œé¡µé¢å¢å¼º
            enhancePage();
            setTimeout(enhancePage, 1500);
        """,
        wait_for="body",
        cache_mode=CacheMode.BYPASS,
        markdown_generator=DefaultMarkdownGenerator(
            content_filter=PruningContentFilter()
        )
    )

    # 3) åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = AsyncWebCrawler(config=browser_config)

    # ç™»å½•çŠ¶æ€ç®¡ç†
    login_status = {
        "is_logged_in": False,
        "cookies_loaded": False
    }

    # Hook: é¡µé¢å’Œä¸Šä¸‹æ–‡åˆ›å»ºå
    async def on_page_context_created(page: Page, context: BrowserContext, **kwargs):
        print("[HOOK] è®¾ç½®é¡µé¢å’Œä¸Šä¸‹æ–‡...")
        
        # è®¾ç½®è¶…æ—¶
        page.set_default_timeout(60000)
        
        # å°è¯•åŠ è½½å·²ä¿å­˜çš„cookie
        if os.path.exists(COOKIE_PATH):
            try:
                with open(COOKIE_PATH, "r", encoding="utf-8") as f:
                    cookies = json.load(f)
                    if cookies and len(cookies) > 0:
                        await context.add_cookies(cookies)
                        print(f"[HOOK] æˆåŠŸåŠ è½½ {len(cookies)} ä¸ªcookie")
                        login_status["cookies_loaded"] = True
                        login_status["is_logged_in"] = True  # å‡è®¾cookieæœ‰æ•ˆ
                    else:
                        print("[HOOK] Cookieæ–‡ä»¶å­˜åœ¨ä½†ä¸ºç©ºæˆ–æ— æ•ˆ")
            except Exception as e:
                print(f"[HOOK] åŠ è½½Cookieå¤±è´¥: {str(e)}")
        
        return page

    # Hook: é¡µé¢å¯¼èˆªå‰
    async def before_goto(page: Page, context: BrowserContext, url: str, **kwargs):
        print(f"[HOOK] å‡†å¤‡è®¿é—®: {url}")
        
        # è®¾ç½®æµè§ˆå™¨å¤´ä¿¡æ¯
        await page.set_extra_http_headers({
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
        })
        
        return page

    # å¤„ç†ç™»å½•çš„å‡½æ•°
    async def handle_login(page: Page, context: BrowserContext):
        print("[HOOK] å°è¯•ç™»å½•çŸ¥è¯†æ˜Ÿçƒ...")
        
        try:
            # ç¡®è®¤åœ¨ç™»å½•é¡µé¢
            if "login" not in page.url and not await page.is_visible(".login-btn, .sign-in, .login-wall"):
                await page.goto(LOGIN_URL)
                await page.wait_for_load_state("networkidle")
            
            # å…ˆå°è¯•ä½¿ç”¨å·²æœ‰cookie
            if os.path.exists(COOKIE_PATH) and os.path.getsize(COOKIE_PATH) > 10:
                try:
                    print("[HOOK] æ£€æµ‹åˆ°cookieæ–‡ä»¶ï¼Œå°è¯•ä½¿ç”¨...")
                    with open(COOKIE_PATH, "r", encoding="utf-8") as f:
                        cookies = json.load(f)
                    
                    if cookies and len(cookies) > 0:
                        await context.add_cookies(cookies)
                        await page.reload()
                        await page.wait_for_load_state("networkidle")
                        
                        # æ£€æŸ¥ç™»å½•çŠ¶æ€
                        await asyncio.sleep(3)
                        if not await page.is_visible(".login-btn, .sign-in, .login-wall"):
                            print("[HOOK] ä½¿ç”¨cookieç™»å½•æˆåŠŸ!")
                            login_status["is_logged_in"] = True
                            return True
                        else:
                            print("[HOOK] ä½¿ç”¨cookieç™»å½•å¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç™»å½•")
                except Exception as e:
                    print(f"[HOOK] ä½¿ç”¨cookieç™»å½•å‡ºé”™: {str(e)}")
            
            # ç­‰å¾…æ‰‹åŠ¨ç™»å½•
            print(f"\n[HOOK] è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•ï¼Œå°†ç­‰å¾…{LOGIN_TIMEOUT}ç§’...")
            print("[HOOK] æ”¯æŒå¾®ä¿¡æ‰«ç ç™»å½•...")
            
            for i in range(LOGIN_TIMEOUT, 0, -5):
                print(f"[HOOK] ç­‰å¾…ç™»å½•...å‰©ä½™{i}ç§’")
                await asyncio.sleep(5)
                
                # æ£€æŸ¥ç™»å½•çŠ¶æ€
                login_success = False
                
                # æ£€æŸ¥URL
                if "login" not in page.url and "/dweb2/" in page.url:
                    login_success = True
                
                # æ£€æŸ¥ç”¨æˆ·å…ƒç´ 
                try:
                    if await page.query_selector(".user-avatar, .username, .user-info"):
                        login_success = True
                except:
                    pass
                
                # æ£€æŸ¥ç™»å½•å…ƒç´ æ˜¯å¦æ¶ˆå¤±
                if not await page.is_visible(".login-btn, .sign-in, .login-wall"):
                    login_success = True
                
                if login_success:
                    print("[HOOK] æ£€æµ‹åˆ°ç™»å½•æˆåŠŸï¼")
                    login_status["is_logged_in"] = True
                    
                    # ä¿å­˜cookies
                    cookies = await context.cookies()
                    with open(COOKIE_PATH, "w", encoding="utf-8") as f:
                        json.dump(cookies, f, ensure_ascii=False, indent=2)
                    print(f"[HOOK] å·²ä¿å­˜cookiesåˆ° {COOKIE_PATH}")
                    
                    return True
            
            print("[HOOK] ç™»å½•è¶…æ—¶ï¼Œè¯·é‡è¯•")
            return False
            
        except Exception as e:
            print(f"[HOOK] ç™»å½•è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            return False

    # Hook: é¡µé¢å¯¼èˆªå
    async def after_goto(page: Page, context: BrowserContext, url: str, response, **kwargs):
        print(f"[HOOK] å·²åŠ è½½é¡µé¢: {url}")
        
        # å¦‚æœæ˜¯çŸ¥è¯†æ˜Ÿçƒç½‘ç«™
        if "zsxq.com" in url:
            await page.wait_for_load_state("networkidle")
            
            # æ£€æŸ¥å†…å®¹æ˜¯å¦å¯è§
            content_visible = await page.is_visible(".article-title, .content, article, .post-content", timeout=3000)
            
            # å¦‚æœå†…å®¹ä¸å¯è§ä¸”æœªç™»å½•ï¼Œè¿›è¡Œç™»å½•
            if not content_visible and not login_status["is_logged_in"]:
                print("[HOOK] å†…å®¹ä¸å¯è§ï¼Œå°è¯•ç™»å½•...")
                await page.goto(LOGIN_URL)
                await page.wait_for_load_state("networkidle")
                
                # ç™»å½•
                await handle_login(page, context)
                
                # ç™»å½•æˆåŠŸåè¿”å›åŸé¡µé¢
                if login_status["is_logged_in"]:
                    print("[HOOK] ç™»å½•æˆåŠŸï¼Œè¿”å›åŸå§‹é¡µé¢")
                    await page.goto(url)
                    await page.wait_for_load_state("networkidle")
            
            # å¦‚æœåŠ è½½äº†cookieä½†å†…å®¹ä¸å¯è§
            elif login_status["cookies_loaded"] and not content_visible:
                print("[HOOK] å·²åŠ è½½Cookieä½†å†…å®¹ä¸å¯è§ï¼Œå¯èƒ½Cookieå·²è¿‡æœŸ")
                await page.reload()
                
                # å†æ¬¡æ£€æŸ¥å†…å®¹
                content_visible_after_refresh = await page.is_visible(".article-title, .content, article, .post-content", timeout=3000)
                if not content_visible_after_refresh:
                    print("[HOOK] åˆ·æ–°åä»æ— æ³•çœ‹åˆ°å†…å®¹ï¼Œå°è¯•é‡æ–°ç™»å½•")
                    login_status["is_logged_in"] = False
                    
                    await page.goto(LOGIN_URL)
                    await page.wait_for_load_state("networkidle")
                    await handle_login(page, context)
                    
                    if login_status["is_logged_in"]:
                        await page.goto(url)
                        await page.wait_for_load_state("networkidle")
            
            # è°ƒè¯•æ¨¡å¼ä¸‹æˆªå›¾
            if DEBUG:
                await page.screenshot(path="page_debug.png", full_page=True)
        
        return page

    # æ³¨å†Œé’©å­å‡½æ•°
    crawler.crawler_strategy.set_hook("on_page_context_created", on_page_context_created)
    crawler.crawler_strategy.set_hook("before_goto", before_goto)
    crawler.crawler_strategy.set_hook("after_goto", after_goto)

    # å¯åŠ¨çˆ¬è™«
    await crawler.start()

    # è¿è¡Œçˆ¬è™«è®¿é—®ç›®æ ‡URL
    result = await crawler.arun(TARGET_URL, config=crawler_run_config)

    if result.success:
        print("\nçˆ¬å–æˆåŠŸï¼")
        print("URL:", result.url)
        print("HTMLé•¿åº¦:", len(result.html))
        print(f"ç™»å½•çŠ¶æ€: {'å·²ç™»å½•' if login_status['is_logged_in'] else 'æœªç™»å½•'}")
        
        # ä¿å­˜HTMLç»“æœ
        with open("crawled_content.html", "w", encoding="utf-8") as f:
            f.write(result.html)
            print("å·²ä¿å­˜HTMLå†…å®¹åˆ° crawled_content.html")        
        
        # ä¿å­˜Markdownç»“æœ
        with open("crawled_content.md", "w", encoding="utf-8") as f:
            f.write(result.markdown.fit_markdown)
            print("å·²ä¿å­˜Markdownå†…å®¹åˆ° crawled_content.md")
    else:
        print("çˆ¬å–å¤±è´¥:", result.error_message)

    await crawler.close()

if __name__ == "__main__":
    asyncio.run(main()) 